<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>TreeEval&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#26641;&#35268;&#21010;&#31574;&#30053;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;</title><link>https://arxiv.org/abs/2402.13125</link><description>&lt;p&gt;
TreeEval&#65306;&#36890;&#36807;&#26641;&#35268;&#21010;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13125
&lt;/p&gt;
&lt;p&gt;
TreeEval&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#26641;&#35268;&#21010;&#31574;&#30053;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24314;&#31435;&#20102;&#35768;&#22810;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#35745;&#31639;&#25972;&#20307;&#24471;&#20998;&#25110;&#20351;&#29992;&#21478;&#19968;&#20010;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#22522;&#20934;&#30340;&#20844;&#24320;&#35775;&#38382;&#21644;&#35780;&#20272;&#36807;&#31243;&#30340;&#19981;&#28789;&#27963;&#32780;&#36973;&#21463;&#25968;&#25454;&#27844;&#28431;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TreeEval&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;&#65292;&#35753;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;LLM&#20027;&#25345;&#19968;&#20010;&#19981;&#21487;&#37325;&#29616;&#30340;&#35780;&#20272;&#20250;&#35805;&#65292;&#20174;&#26681;&#26412;&#19978;&#36991;&#20813;&#20102;&#25968;&#25454;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;LLM&#20805;&#24403;&#19968;&#20010;&#32771;&#23448;&#65292;&#25552;&#20986;&#19968;&#31995;&#21015;&#20851;&#20110;&#19968;&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#26641;&#35268;&#21010;&#31574;&#30053;&#65292;&#32771;&#34385;&#24403;&#21069;&#30340;&#35780;&#20272;&#29366;&#24577;&#26469;&#20915;&#23450;&#19979;&#19968;&#20010;&#38382;&#39064;&#30340;&#29983;&#25104;&#65292;&#30830;&#20445;&#35780;&#20272;&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;6&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;7B&#12289;13B&#21644;33B&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#30456;&#20851;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13125v1 Announce Type: cross  Abstract: Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coef
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#23398;&#26415;&#39046;&#22495;&#22312;43&#24180;&#38388;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#30340;&#36235;&#21183;&#65292;&#21457;&#29616;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#24341;&#25991;&#24180;&#40836;&#34928;&#36864;&#26368;&#20026;&#26126;&#26174;&#65292;&#27492;&#36235;&#21183;&#24182;&#38750;&#30001;&#20986;&#29256;&#36895;&#29575;&#22686;&#38271;&#20027;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.12046</link><description>&lt;p&gt;
&#24341;&#25991;&#36951;&#24536;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#23398;&#26415;&#39046;&#22495;&#27491;&#22788;&#20110;&#24341;&#25991;&#24180;&#40836;&#34928;&#36864;&#26399;
&lt;/p&gt;
&lt;p&gt;
Citation Amnesia: NLP and Other Academic Fields Are in a Citation Age Recession
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12046
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#23398;&#26415;&#39046;&#22495;&#22312;43&#24180;&#38388;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#30340;&#36235;&#21183;&#65292;&#21457;&#29616;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#24341;&#25991;&#24180;&#40836;&#34928;&#36864;&#26368;&#20026;&#26126;&#26174;&#65292;&#27492;&#36235;&#21183;&#24182;&#38750;&#30001;&#20986;&#29256;&#36895;&#29575;&#22686;&#38271;&#20027;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#23519;&#20102;&#22312;43&#24180;&#30340;&#26102;&#38388;&#36328;&#24230;&#65288;1980-2023&#24180;&#65289;&#20869;&#65292;&#22312;20&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#20542;&#21521;&#20110;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20542;&#21521;&#20110;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#30340;&#29305;&#24615;&#25918;&#22312;&#20854;&#20182;20&#20010;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#25506;&#35752;NLP&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20854;&#20182;&#39046;&#22495;&#38543;&#26102;&#38388;&#20986;&#29616;&#31867;&#20284;&#30340;&#24341;&#25991;&#27169;&#24335;&#65292;&#25110;&#32773;&#26159;&#21542;&#21487;&#20197;&#35266;&#23519;&#21040;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#32422;2.4&#20159;&#31687;&#35770;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#36235;&#21183;&#65306;&#35768;&#22810;&#39046;&#22495;&#22312;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#26041;&#38754;&#26126;&#26174;&#19979;&#38477;&#65288;&#20363;&#22914;&#24515;&#29702;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#19979;&#38477;&#31216;&#20026;&#8220;&#24341;&#25991;&#24180;&#40836;&#34928;&#36864;&#8221;&#65292;&#31867;&#20284;&#20110;&#32463;&#27982;&#23398;&#23478;&#22914;&#20309;&#23450;&#20041;&#20943;&#23569;&#32463;&#27982;&#27963;&#21160;&#30340;&#26102;&#26399;&#12290;&#36825;&#19968;&#36235;&#21183;&#22312;NLP&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#20026;&#26174;&#33879;&#65288;&#24341;&#25991;&#24180;&#40836;&#20174;&#20808;&#21069;&#39640;&#23792;&#19979;&#38477;&#20102;12.8%&#21644;5.5%&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#26356;&#36817;&#26399;&#20316;&#21697;&#30340;&#24341;&#29992;&#24182;&#38750;&#30452;&#25509;&#21463;&#21040;&#20986;&#29256;&#36895;&#29575;&#22686;&#38271;&#30340;&#25512;&#21160;&#65288;&#36328;&#39046;&#22495;&#19979;&#38477;&#20102;3.4%&#65292;&#20154;&#25991;&#23398;&#31185;&#19979;&#38477;&#20102;5.2%&#65292;&#24418;&#24335;&#31185;&#23398;&#19979;&#38477;&#20102;5.5%&#65289;--&#21363;&#20351;&#22312;&#25511;&#21046;&#20102;&#21457;&#34920;&#25968;&#37327;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12046v1 Announce Type: cross  Abstract: This study examines the tendency to cite older work across 20 fields of study over 43 years (1980--2023). We put NLP's propensity to cite older work in the context of these 20 other fields to analyze whether NLP shows similar temporal citation patterns to these other fields over time or whether differences can be observed. Our analysis, based on a dataset of approximately 240 million papers, reveals a broader scientific trend: many fields have markedly declined in citing older works (e.g., psychology, computer science). We term this decline a 'citation age recession', analogous to how economists define periods of reduced economic activity. The trend is strongest in NLP and ML research (-12.8% and -5.5% in citation age from previous peaks). Our results suggest that citing more recent works is not directly driven by the growth in publication rates (-3.4% across fields; -5.2% in humanities; -5.5% in formal sciences) -- even when controlli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PLURAL&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20013;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08966</link><description>&lt;p&gt;
&#29992;&#20110;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20013;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PLURAL&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20013;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;(diff-VQA)&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#26681;&#25454;&#19968;&#23545;&#22270;&#20687;&#30340;&#24046;&#24322;&#22238;&#31572;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#22312;&#35835;&#21462;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#36890;&#24120;&#20250;&#23545;&#21516;&#19968;&#24739;&#32773;&#22312;&#19981;&#21516;&#26102;&#38388;&#25293;&#25668;&#30340;&#22810;&#24133;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#36861;&#36394;&#30142;&#30149;&#30340;&#36827;&#23637;&#21644;&#20854;&#20020;&#24202;&#23454;&#36341;&#20013;&#20005;&#37325;&#31243;&#24230;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20026;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#35774;&#35745;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#38169;&#36807;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLM)&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PLURAL&#30340;&#26032;&#22411;VLM&#65292;&#23427;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#36880;&#27493;&#30340;&#26041;&#27861;&#24320;&#21457;&#65292;&#20174;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#32437;&#21521;&#25968;&#25454;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08966v1 Announce Type: cross Abstract: Difference visual question answering (diff-VQA) is a challenging task that requires answering complex questions based on differences between a pair of images. This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice. However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model's performance using a pretrained vision-language model (VLM). Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task. The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data. The longitudinal data consist
&lt;/p&gt;</description></item><item><title>ReFT&#26159;&#19968;&#31181;&#21152;&#24378;&#25512;&#29702;&#33021;&#21147;&#30340;&#24378;&#21270;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08967</link><description>&lt;p&gt;
ReFT: &#21152;&#24378;&#24378;&#21270;&#24494;&#35843;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReFT: Reasoning with Reinforced Fine-Tuning. (arXiv:2401.08967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08967
&lt;/p&gt;
&lt;p&gt;
ReFT&#26159;&#19968;&#31181;&#21152;&#24378;&#25512;&#29702;&#33021;&#21147;&#30340;&#24378;&#21270;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#27880;&#37322;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#24182;&#19981;&#21313;&#20998;&#24378;&#22823;&#65292;&#22240;&#20026;&#35757;&#32451;&#20165;&#20381;&#36182;&#20110;&#32473;&#23450;&#30340;CoT&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#36890;&#24120;&#21482;&#26377;&#19968;&#20010;&#27880;&#37322;&#30340;&#25512;&#29702;&#36335;&#24452;&#29992;&#20110;&#27599;&#20010;&#38382;&#39064;&#12290;&#30452;&#35266;&#26469;&#35828;&#65292;&#35753;&#31639;&#27861;&#20174;&#32473;&#23450;&#30340;&#38382;&#39064;&#20013;&#23398;&#20064;&#22810;&#20010;&#27880;&#37322;&#30340;&#25512;&#29702;&#36335;&#24452;&#20250;&#26356;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21152;&#24378;&#24378;&#21270;&#24494;&#35843;&#65288;ReFT&#65289;&#65292;&#20197;&#22686;&#24378;&#23398;&#20064;LLMs&#36827;&#34892;&#25512;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20026;&#20363;&#12290;ReFT&#39318;&#20808;&#20351;&#29992;SFT&#23545;&#27169;&#22411;&#36827;&#34892;&#28909;&#36523;&#65292;&#28982;&#21518;&#37319;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#26412;&#25991;&#20013;&#26159;&#20351;&#29992;PPO&#31639;&#27861;&#65289;&#36827;&#19968;&#27493;&#24494;&#35843;&#27169;&#22411;&#65292;&#20854;&#20013;&#26681;&#25454;&#38382;&#39064;&#33258;&#21160;&#37319;&#26679;&#20102;&#22823;&#37327;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#20845;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#25361;&#25112;&#20013;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#23454;&#35777;&#21457;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#23545;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#24182;&#25193;&#23637;&#32763;&#35793;&#25991;&#26723;&#30340;&#38271;&#24230;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#32597;&#35265;&#35789;&#39044;&#27979;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08350</link><description>&lt;p&gt;
&#21521;&#32463;&#20856;&#33268;&#25964;&#65306;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#26426;&#22120;&#32763;&#35793;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models. (arXiv:2401.08350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#20845;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#25361;&#25112;&#20013;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#23454;&#35777;&#21457;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#23545;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#24182;&#25193;&#23637;&#32763;&#35793;&#25991;&#26723;&#30340;&#38271;&#24230;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#32597;&#35265;&#35789;&#39044;&#27979;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793; (NMT) &#30340;&#21457;&#23637;&#21463;&#21040;&#20845;&#20010;&#26680;&#24515;&#25361;&#25112;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#36825;&#20123;&#25361;&#25112;&#20026;&#36825;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#20808;&#36827;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#32972;&#26223;&#19979;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#25361;&#25112;&#25345;&#32493;&#30456;&#20851;&#24615;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;LLM&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#23545;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20027;&#35201;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;LLM&#30340;&#32763;&#35793;&#31995;&#32479;&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#32422;80&#20010;&#21333;&#35789;&#30340;&#38271;&#21477;&#23376;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#32763;&#35793;&#38271;&#36798;512&#20010;&#21333;&#35789;&#30340;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#32597;&#35265;&#35789;&#39044;&#27979;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;&#22312;&#35299;&#20915;&#21333;&#35789;&#23545;&#40784;&#21644;&#20122;&#26368;&#20248;&#25628;&#32034;&#30340;&#25361;&#25112;&#26041;&#38754;&#65292;LLM&#20173;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017), which have acted as benchmarks for progress in this field. This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch, amount of parallel data, rare word prediction, translation of long sentences, attention model as word alignment, and sub-optimal beam search. Our empirical findings indicate that LLMs effectively lessen the reliance on parallel data for major languages in the pretraining phase. Additionally, the LLM-based translation system significantly enhances the translation of long sentences that contain approximately 80 words and shows the capability to translate documents of up to 512 words. However, despite these significant improvements, the challenges of domain mismatch and prediction of rare words persist. While the challenges of word alignment a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#20811;&#38534;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20174;CodeForces&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;XCD&#12290;&#25105;&#20204;&#20351;&#29992;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;CCT&#65289;&#26041;&#27861;&#35757;&#32451;&#20102;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#20855;&#26377;&#26032;&#39062;&#24615;&#33021;&#30340;CCT-LM&#27169;&#22411;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11626</link><description>&lt;p&gt;
CCT-Code&#65306;&#38754;&#21521;&#22810;&#35821;&#35328;&#20811;&#38534;&#26816;&#27979;&#21644;&#20195;&#30721;&#25628;&#32034;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search. (arXiv:2305.11626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#20811;&#38534;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20174;CodeForces&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;XCD&#12290;&#25105;&#20204;&#20351;&#29992;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;CCT&#65289;&#26041;&#27861;&#35757;&#32451;&#20102;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#20855;&#26377;&#26032;&#39062;&#24615;&#33021;&#30340;CCT-LM&#27169;&#22411;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#28304;&#20195;&#30721;&#30340;&#20811;&#38534;&#26816;&#27979;&#21644;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#38382;&#39064;&#23545;&#20110;&#20219;&#20309;&#32534;&#31243;&#35821;&#35328;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#20811;&#38534;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20174;CodeForces&#25552;&#20132;&#25968;&#25454;&#38598;&#20135;&#29983;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;XCD&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;CCT&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36827;&#32780;&#24471;&#21040;&#22522;&#20110;CCT-LM &#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32487;&#25215;&#20102;GraphCodeBERT&#24182;&#29992;CCT&#24494;&#35843;&#65292;&#36798;&#21040;&#20102;95.67\% MAP&#21644;47.18\% MRR&#30340;&#24615;&#33021;&#65292;&#25104;&#21151;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the clone detection and information retrieval problems for source code, well-known tasks important for any programming language. Although it is also an important and interesting problem to find code snippets that operate identically but are written in different programming languages, to the best of our knowledge multilingual clone detection has not been studied in literature. In this work, we formulate the multilingual clone detection problem and present XCD, a new benchmark dataset produced from the CodeForces submissions dataset. Moreover, we present a novel training procedure, called cross-consistency training (CCT), that we apply to train language models on source code in different programming languages. The resulting CCT-LM model, initialized with GraphCodeBERT and fine-tuned with CCT, achieves new state of the art, outperforming existing approaches on the POJ-104 clone detection benchmark with 95.67\% MAP and AdvTest code search benchmark with 47.18\% MRR; it also sho
&lt;/p&gt;</description></item></channel></rss>