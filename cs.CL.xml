<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07714</link><description>&lt;p&gt;
StableToolBench&#65306;&#38754;&#21521;&#22823;&#35268;&#27169;&#31283;&#23450;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20855;&#23398;&#20064;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07714
&lt;/p&gt;
&lt;p&gt;
StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#20351;&#20154;&#20204;&#25506;&#32034;&#24037;&#20855;&#23398;&#20064;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#24037;&#20855;&#25972;&#21512;&#20197;&#35299;&#20915;&#21508;&#31181;&#29616;&#23454;&#25361;&#25112;&#12290;&#35780;&#20272;LLMs&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#38656;&#35201;&#22823;&#35268;&#27169;&#19988;&#31283;&#23450;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;ToolBench&#28436;&#21464;&#32780;&#26469;&#30340;StableToolBench&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#12290;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21253;&#21547;&#32531;&#23384;&#31995;&#32479;&#21644;API&#27169;&#25311;&#22120;&#65292;&#20114;&#34917;&#20943;&#36731;API&#29366;&#24577;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#31283;&#23450;&#30340;&#35780;&#20272;&#31995;&#32479;&#20351;&#29992;GPT-4&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#35774;&#35745;&#21487;&#35299;&#20915;&#30340;&#36890;&#36807;&#29575;&#21644;&#32988;&#29575;&#65292;&#20197;&#28040;&#38500;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07714v1 Announce Type: new  Abstract: Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#19987;&#23478;&#25552;&#31034;&#8221;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#24182;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#36825;&#20123;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#65292;&#35813;&#21161;&#25163;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;96&#65285;&#30340;ChatGPT&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14688</link><description>&lt;p&gt;
&#19987;&#23478;&#25552;&#31034;&#65306;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. (arXiv:2305.14688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#19987;&#23478;&#25552;&#31034;&#8221;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#24182;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#36825;&#20123;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#65292;&#35813;&#21161;&#25163;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;96&#65285;&#30340;ChatGPT&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20197;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22238;&#31572;&#36136;&#37327;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#25552;&#31034;&#65292;&#20197;&#24341;&#21457;LLMs&#20316;&#20026;&#26480;&#20986;&#19987;&#23478;&#22238;&#31572;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#29305;&#23450;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#28982;&#21518;&#35201;&#27714;LLMs&#26681;&#25454;&#36825;&#31181;&#20195;&#29702;&#20154;&#32972;&#26223;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#22686;&#24378;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#31454;&#20105;&#24615;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;GPT4&#30340;&#35780;&#20272;&#26174;&#31034;&#65306;1&#65289;&#19987;&#23478;&#25968;&#25454;&#30340;&#36136;&#37327;&#26174;&#33879;&#39640;&#20110;&#26222;&#36890;&#31572;&#26696;&#65292;2&#65289;ExpertLLaMA&#32988;&#36807;&#29616;&#26377;&#30340;&#24320;&#28304;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;ChatGPT&#33021;&#21147;&#30340;96&#65285;&#12290;&#25152;&#26377;&#25968;&#25454;&#21644;ExpertLLaMA&#27169;&#22411;&#23558;&#22312;\url{https://github.com/OFA-Sys/Exp}&#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \url{https://github.com/OFA-Sys/Exp
&lt;/p&gt;</description></item></channel></rss>