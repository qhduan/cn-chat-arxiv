<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22238;&#22768;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36755;&#20837;&#26469;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#21253;&#21547;&#21518;&#32493;&#20196;&#29260;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#20805;&#20998;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.15449</link><description>&lt;p&gt;
&#37325;&#22797;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Repetition Improves Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15449
&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36755;&#20837;&#26469;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#21253;&#21547;&#21518;&#32493;&#20196;&#29260;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#20805;&#20998;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25913;&#36827;&#20174;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#25968;&#25454;&#12289;&#39592;&#24178;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25110;&#36890;&#36807;&#25351;&#20196;&#25913;&#36827;&#20219;&#21153;&#24046;&#24322;&#21270;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19968;&#20010;&#26550;&#26500;&#38480;&#21046;&#65306;&#20196;&#29260;&#23884;&#20837;&#19981;&#33021;&#21253;&#21547;&#26469;&#33258;&#36755;&#20837;&#20013;&#21518;&#32493;&#20196;&#29260;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#8220;&#22238;&#22768;&#23884;&#20837;&#8221;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#19978;&#19979;&#25991;&#20013;&#23558;&#36755;&#20837;&#37325;&#22797;&#20004;&#27425;&#65292;&#24182;&#20174;&#31532;&#20108;&#27425;&#20986;&#29616;&#20013;&#25552;&#21462;&#23884;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26089;&#26399;&#20196;&#29260;&#30340;&#22238;&#22768;&#23884;&#20837;&#21487;&#20197;&#32534;&#30721;&#20851;&#20110;&#21518;&#32493;&#20196;&#29260;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;LLMs&#36827;&#34892;&#23884;&#20837;&#12290;&#22312;MTEB&#25490;&#34892;&#27036;&#19978;&#65292;&#22238;&#22768;&#23884;&#20837;&#22312;&#38646;&#23556;&#20987;&#20013;&#27604;&#32463;&#20856;&#23884;&#20837;&#25552;&#39640;&#20102;&#36229;&#36807;9%&#65292;&#22312;&#24494;&#35843;&#26102;&#25552;&#39640;&#20102;&#32422;0.7%&#12290;&#20351;&#29992;Mistral-7B&#27169;&#22411;&#30340;&#22238;&#22768;&#23884;&#20837;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15449v1 Announce Type: new  Abstract: Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, "echo embeddings," in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>&#27721;&#35821;&#20013;&#32570;&#22833;NP&#30340;&#21452;&#37325;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#24182;&#19981;&#26159;&#35821;&#27861;&#24615;&#38169;&#35273;&#65292;&#32780;&#26159;&#21160;&#35789;&#27495;&#20041;&#35299;&#37322;&#30340;&#21547;&#31946;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.11282</link><description>&lt;p&gt;
&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#20013;&#30340;&#32570;&#22833;NP&#25928;&#24212;&#30340;&#24615;&#36136;&#65306;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#25581;&#31034;&#26197;&#30505;&#38169;&#35273;&#36824;&#26159;&#21547;&#31946;&#35299;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Grammaticality illusion or ambiguous interpretation? Event-related potentials reveal the nature of the missing-NP effect in Mandarin centre-embedded structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11282
&lt;/p&gt;
&lt;p&gt;
&#27721;&#35821;&#20013;&#32570;&#22833;NP&#30340;&#21452;&#37325;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#24182;&#19981;&#26159;&#35821;&#27861;&#24615;&#38169;&#35273;&#65292;&#32780;&#26159;&#21160;&#35789;&#27495;&#20041;&#35299;&#37322;&#30340;&#21547;&#31946;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20960;&#31181;&#35821;&#35328;&#20013;&#65292;&#22312;&#21452;&#37325;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#20013;&#30465;&#30053;&#21160;&#35789;&#30701;&#35821;&#65288;VP&#65289;&#20250;&#20135;&#29983;&#19968;&#20010;&#35821;&#27861;&#24615;&#38169;&#35273;&#12290;&#31867;&#20284;&#30340;&#38169;&#35273;&#20063;&#34920;&#29616;&#22312;&#27721;&#35821;&#32570;&#22833;NP&#30340;&#21452;&#37325;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#20013;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#30340;&#26412;&#36136;&#23578;&#26080;&#20849;&#35782;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#20854;&#25226;&#23427;&#30475;&#20316;&#26159;&#35821;&#27861;&#24615;&#38169;&#35273;&#65292;&#19981;&#22914;&#23558;&#21160;&#35789;&#30340;&#27495;&#20041;&#35299;&#37322;&#35270;&#20026;&#26368;&#33021;&#35299;&#37322;&#27721;&#35821;&#20013;&#36825;&#19968;&#29616;&#35937;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#20943;&#23569;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#20004;&#39033;&#19982;&#33258;&#23884;&#20837;&#20851;&#31995;&#20174;&#21477;&#25918;&#32622;&#22312;&#21477;&#23376;&#20027;&#35821;&#20301;&#32622;&#30456;&#32467;&#21512;&#30456;&#36817;&#21452;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#23454;&#39564;&#12290;&#23454;&#39564;1&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#32467;&#26500;&#20013;&#21516;&#26679;&#20250;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#29616;&#35937;&#65292;&#35777;&#25454;&#26159;&#32570;&#23569;P600&#25928;&#24212;&#32780;&#23384;&#22312;N400&#25928;&#24212;&#12290;&#22312;&#23454;&#39564;2&#20013;&#65292;&#36890;&#36807;&#25552;&#20379;&#35821;&#20041;&#32447;&#32034;&#20197;&#20943;&#23569;&#27495;&#20041;&#65292;&#28040;&#38500;&#20102;&#36825;&#31181;&#38169;&#35273;&#65292;&#35777;&#25454;&#26159;&#23384;&#22312;P600&#25928;&#24212;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#20123;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11282v1 Announce Type: new  Abstract: In several languages, omitting a verb phrase (VP) in double centre-embedded structures creates a grammaticality illusion. Similar illusion also exhibited in Mandarin missing-NP double centre-embedded structures. However, there is no consensus on its very nature. Instead of treating it as grammaticality illusion, we argue that ambiguous interpretations of verbs can best account for this phenomenon in Mandarin. To further support this hypothesis, we conducted two electroencephalography (EEG) experiments on quasi double centre-embedded structures whose complexity is reduced by placing the self-embedding relative clauses into the sentence's subject position. Experiment 1 showed that similar phenomenon even exhibited in this structure, evidenced by an absence of P600 effect and a presence of N400 effect. In Experiment 2, providing semantic cues to reduce ambiguity dispelled this illusion, as evidenced by a P600 effect. We interpret the result
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01766</link><description>&lt;p&gt;
&#25903;&#25345;&#36824;&#26159;&#21453;&#39539;&#65306;&#20998;&#26512;&#35777;&#25454;&#31435;&#22330;&#20197;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35823;&#23548;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#22269;&#23478;&#32423;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#26159;&#21508;&#31181;&#22312;&#32447;&#20260;&#23475;&#30340;&#20027;&#35201;&#26469;&#28304;&#20043;&#19968;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#35823;&#23548;&#20449;&#24687;&#24418;&#24335;&#26159;&#19978;&#19979;&#25991;&#38169;&#35823;&#65288;OOC&#65289;&#20449;&#24687;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#20449;&#24687;&#34987;&#38169;&#35823;&#22320;&#20851;&#32852;&#36215;&#26469;&#65292;&#20363;&#22914;&#30495;&#23454;&#22270;&#20687;&#19982;&#34394;&#20551;&#30340;&#25991;&#26412;&#26631;&#39064;&#25110;&#35823;&#23548;&#24615;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#22806;&#37096;&#35777;&#25454;&#26469;&#25269;&#24481;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#31435;&#22330;&#30340;&#19981;&#21516;&#35777;&#25454;&#30340;&#20316;&#29992;&#12290;&#21463;&#21040;&#35777;&#25454;&#31435;&#22330;&#20195;&#34920;&#19981;&#21516;&#26816;&#27979;&#32467;&#26524;&#30340;&#20559;&#35265;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#25552;&#21462;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#20849;&#29616;&#20851;&#31995;&#35745;&#31639;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#21040;&#25991;&#26412;SEN&#20013;&#12290;&#23545;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14394</link><description>&lt;p&gt;
&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#38388;&#32763;&#35793;&#28041;&#21450;&#22312;&#32473;&#23450;&#28304;&#22495;&#26465;&#20214;&#19979;&#29983;&#25104;&#30446;&#26631;&#22495;&#26679;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22266;&#23450;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#19978;&#65292;&#21363;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#37197;&#32622;&#65288;&#20363;&#22914;&#23545;&#20110;&#20004;&#20010;&#22495;&#65292;&#35201;&#20040;$D_1\rightarrow{}D_2$&#65292;&#35201;&#20040;$D_2\rightarrow{}D_1$&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Multi-Domain Diffusion&#65288;MDD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#30340;&#26465;&#20214;&#25193;&#25955;&#26694;&#26550;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MDD&#19981;&#38656;&#35201;&#23450;&#20041;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#65292;&#20801;&#35768;&#22312;&#19968;&#32452;&#22495;&#30340;&#20219;&#20309;&#20998;&#21306;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65288;&#20363;&#22914;$(D_1, D_2)\rightarrow{}D_3$&#65292;$D_2\rightarrow{}(D_1, D_3)$&#65292;$D_3\rightarrow{}D_1$&#31561;&#65289;&#65292;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;&#22495;&#37197;&#32622;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;MDD&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#24418;&#24335;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22495;&#24341;&#20837;&#19968;&#20010;&#22122;&#22768;&#32423;&#21035;&#65292;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#23558;&#20256;&#32479;&#30340;&#32763;&#35793;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#36890;&#36807;&#22122;&#22768;&#24314;&#27169;&#26469;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-to-domain translation involves generating a target domain sample given a condition in the source domain. Most existing methods focus on fixed input and output domains, i.e. they only work for specific configurations (i.e. for two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for multi-domain translation in a semi-supervised context. Unlike previous methods, MDD does not require defining input and output domains, allowing translation between any partition of domains within a set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for 3 domains), without the need to train separate models for each domain configuration. The key idea behind MDD is to leverage the noise formulation of diffusion models by incorporating one noise level per domain, which allows missing domains to be modeled with noise in a natural way. This transforms the tra
&lt;/p&gt;</description></item></channel></rss>