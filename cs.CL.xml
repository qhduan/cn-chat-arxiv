<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Nomic Embed&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;</title><link>https://rss.arxiv.org/abs/2402.01613</link><description>&lt;p&gt;
Nomic Embed&#65306;&#35757;&#32451;&#21487;&#22797;&#29616;&#30340;&#38271;&#19978;&#19979;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nomic Embed: Training a Reproducible Long Context Text Embedder
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01613
&lt;/p&gt;
&lt;p&gt;
Nomic Embed&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;nomic-embed-text-v1&#30340;&#35757;&#32451;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;&#25105;&#20204;&#22312;Apache 2&#35768;&#21487;&#19979;&#21457;&#24067;&#20102;&#35757;&#32451;&#20195;&#30721;&#21644;&#27169;&#22411;&#26435;&#37325;&#12290;&#19982;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;2.35&#20159;&#20010;&#31574;&#21010;&#25991;&#26412;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#21152;&#36733;&#22120;&#65292;&#21487;&#20197;&#23436;&#20840;&#22797;&#29616;nomic-embed-text-v1&#12290;&#20320;&#21487;&#20197;&#22312;https://github.com/nomic-ai/contrastors&#25214;&#21040;&#27169;&#22411;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors
&lt;/p&gt;</description></item><item><title>StyleSinger&#26159;&#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#30340;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#65292;&#36890;&#36807;&#27531;&#24046;&#39118;&#26684;&#36866;&#37197;&#22120;&#65288;RSA&#65289;&#25429;&#25417;&#22810;&#26679;&#30340;&#39118;&#26684;&#29305;&#24449;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#28436;&#21809;&#22768;&#38899;&#12290;</title><link>http://arxiv.org/abs/2312.10741</link><description>&lt;p&gt;
StyleSinger: &#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#30340;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis. (arXiv:2312.10741v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10741
&lt;/p&gt;
&lt;p&gt;
StyleSinger&#26159;&#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#30340;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#65292;&#36890;&#36807;&#27531;&#24046;&#39118;&#26684;&#36866;&#37197;&#22120;&#65288;RSA&#65289;&#25429;&#25417;&#22810;&#26679;&#30340;&#39118;&#26684;&#29305;&#24449;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#28436;&#21809;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#65288;SVS&#65289;&#30340;&#39118;&#26684;&#36716;&#31227;&#19987;&#27880;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#28436;&#21809;&#22768;&#38899;&#65292;&#35813;&#22768;&#38899;&#20855;&#26377;&#20174;&#21442;&#32771;&#28436;&#21809;&#22768;&#38899;&#26679;&#26412;&#20013;&#34893;&#29983;&#30340;&#26410;&#35265;&#39118;&#26684;&#65288;&#22914;&#38899;&#33394;&#12289;&#24773;&#24863;&#12289;&#21457;&#38899;&#21644;&#21457;&#38899;&#25216;&#24039;&#65289;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#28436;&#21809;&#22768;&#38899;&#39118;&#26684;&#30340;&#31934;&#32454;&#24046;&#24322;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28436;&#21809;&#22768;&#38899;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#34920;&#29616;&#21147;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;SVS&#26041;&#27861;&#22312;&#39046;&#22495;&#22806;&#22330;&#26223;&#20013;&#21512;&#25104;&#30340;&#28436;&#21809;&#22768;&#38899;&#36136;&#37327;&#19979;&#38477;&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#35757;&#32451;&#38454;&#27573;&#21487;&#36776;&#21035;&#20986;&#30446;&#26631;&#22768;&#38899;&#23646;&#24615;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;StyleSinger&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39046;&#22495;&#22806;&#21442;&#32771;&#28436;&#21809;&#22768;&#38899;&#26679;&#26412;&#30340;&#38646;&#26679;&#24335;&#36716;&#31227;&#30340;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;StyleSinger&#37319;&#29992;&#20102;&#20004;&#31181;&#20851;&#38190;&#26041;&#27861;&#20197;&#25552;&#39640;&#25928;&#26524;&#65306;1&#65289;&#27531;&#24046;&#39118;&#26684;&#36866;&#37197;&#22120;&#65288;RSA&#65289;&#65292;&#23427;&#20351;&#29992;&#27531;&#24046;&#37327;&#21270;&#27169;&#22359;&#26469;&#25429;&#25417;&#22810;&#26679;&#30340;&#39118;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses on generating high-quality singing voices with unseen styles (such as timbre, emotion, pronunciation, and articulation skills) derived from reference singing voice samples. However, the endeavor to model the intricate nuances of singing voice styles is an arduous task, as singing voices possess a remarkable degree of expressiveness. Moreover, existing SVS methods encounter a decline in the quality of synthesized singing voices in OOD scenarios, as they rest upon the assumption that the target vocal attributes are discernible during the training phase. To overcome these challenges, we propose StyleSinger, the first singing voice synthesis model for zero-shot style transfer of out-of-domain reference singing voice samples. StyleSinger incorporates two critical approaches for enhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a residual quantization module to capture diverse style character
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.00948</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20998;&#26512;LLM&#30340;&#29702;&#35770;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs. (arXiv:2305.00948v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#26368;&#36817;&#24050;&#32463;&#25552;&#39640;&#21040;&#20102;&#33021;&#22815;&#22312;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#30340;&#24418;&#24335;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#20803;&#35821;&#35328;&#33021;&#21147;&#20998;&#26512;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;LLMs&#20027;&#35201;&#26159;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#30340;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#25913;&#36827;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;&#35821;&#35328;&#23398;&#20013;&#30340;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#27880;&#20110;&#24418;&#24335;&#35821;&#35328;&#23398;&#30340;&#19977;&#20010;&#23376;&#39046;&#22495;&#65306;&#21477;&#27861;&#12289;&#38899;&#38901;&#23398;&#21644;&#35821;&#20041;&#23398;&#65292;&#25506;&#31350;&#20102;GPT-4&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20803;&#35821;&#35328;&#20998;&#26512;&#30340;&#30740;&#31350;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#23454;&#39564;&#35774;&#35745;&#65292;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26041;&#38024;&#65292;&#35752;&#35770;&#20102;&#38480;&#21046;&#65292;&#24182;&#20026;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#36825;&#20010;&#30740;&#31350;&#36824;&#26377;&#21161;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#21644;&#29702;&#35770;&#27169;&#22411;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) has recently improved to the point where the models can perform well on many language tasks. We show here that for the first time, the models can also generate coherent and valid formal analyses of linguistic data and illustrate the vast potential of large language models for analyses of their metalinguistic abilities. LLMs are primarily trained on language data in the form of text; analyzing and evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. In this paper, we probe into GPT-4's metalinguistic capabilities by focusing on three subfields of formal linguistics: syntax, phonology, and semantics. We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research. This line of inquiry als
&lt;/p&gt;</description></item></channel></rss>