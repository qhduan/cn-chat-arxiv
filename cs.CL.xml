<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#32467;&#21512;&#21040;LLMs&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;</title><link>https://arxiv.org/abs/2404.01129</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20449;&#24687;&#24456;&#37325;&#35201;&#65306;&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#24341;&#20837;LLMs&#20197;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01129
&lt;/p&gt;
&lt;p&gt;
&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#32467;&#21512;&#21040;LLMs&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01129v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#33258;&#21160;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#24050;&#32463;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21487;&#35757;&#32451;&#30340;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#26159;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;&#30495;&#27491;&#27491;&#20363;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#36127;&#20363;&#22238;&#22797;&#26469;&#35757;&#32451;&#30340;&#65292;&#23548;&#33268;&#23427;&#20204;&#20542;&#21521;&#20110;&#23558;&#26356;&#39640;&#20869;&#23481;&#30456;&#20284;&#24615;&#30340;&#22238;&#22797;&#20998;&#37197;&#26356;&#39640;&#30340;&#24471;&#20998;&#32473;&#23450;&#19968;&#20010;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#24615;&#30340;&#36127;&#38754;&#22238;&#22797;&#20855;&#26377;&#19982;&#19978;&#19979;&#25991;&#39640;&#20869;&#23481;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#22312;&#35821;&#20041;&#19978;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#35780;&#20272;&#36825;&#31867;&#22238;&#22797;&#65292;&#23548;&#33268;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#26041;&#38754;&#26377;&#19968;&#23450;&#25928;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#24615;&#36127;&#38754;&#31034;&#20363;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#65292;&#23427;&#32467;&#21512;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01129v1 Announce Type: new  Abstract: Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.16393</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#65288;CLED&#65289;
&lt;/p&gt;
&lt;p&gt;
Concurrent Linguistic Error Detection (CLED) for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#37319;&#29992;&#20351;&#24471;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#38169;&#35823;&#30340;&#26816;&#27979;&#26159;&#20943;&#36731;&#20854;&#23545;&#31995;&#32479;&#24433;&#21709;&#30340;&#31532;&#19968;&#27493;&#65292;&#22240;&#27492;&#65292;LLMs&#30340;&#39640;&#25928;&#38169;&#35823;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22522;&#20110;&#23545;LLMs&#36755;&#20986;&#36827;&#34892;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#36827;&#34892;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#65288;CLED&#65289;&#65307;&#35813;&#26041;&#26696;&#25552;&#21462;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#19968;&#20123;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#36755;&#20837;&#21040;&#19968;&#20010;&#24182;&#21457;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16393v1 Announce Type: new  Abstract: The wide adoption of Large language models (LLMs) makes their dependability a pressing concern. Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue. In many settings, the LLM is considered as a black box with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model's internal nodes. An interesting observation is that the output of LLMs in error-free operation should be valid and normal text. Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error. Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the LLM and feeds them to a concurrent classifier that detects errors. Since the proposed error detection mechanism only 
&lt;/p&gt;</description></item></channel></rss>