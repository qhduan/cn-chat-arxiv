<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15796</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding Emergent Abilities of Language Models from the Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#36136;&#30097;&#20102;&#20256;&#32479;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#20165;&#23384;&#22312;&#20110;&#22823;&#27169;&#22411;&#20013;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#24576;&#30097;&#28304;&#33258;&#20004;&#28857;&#35266;&#23519;&#65306;1&#65289;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#23637;&#29616;&#20986;&#23545;&#31361;&#29616;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#65307;2&#65289;&#36136;&#30097;&#29992;&#20110;&#27979;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#19981;&#36830;&#32493;&#24615;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#35758;&#20174;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#35282;&#24230;&#30740;&#31350;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#38750;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#20294;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#26576;&#19968;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#19981;&#35770;&#25351;&#26631;&#30340;&#36830;&#32493;&#24615;&#22914;&#20309;&#65307;&#32780;&#22312;&#36798;&#21040;&#35813;&#38408;&#20540;&#20043;&#21069;&#65292;&#20854;&#24615;&#33021;&#20173;&#20445;&#25345;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#31361;&#29616;&#33021;&#21147;&#20026;&#37027;&#20123;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;</title><link>https://arxiv.org/abs/2403.10700</link><description>&lt;p&gt;
&#27880;&#24847;&#38169;&#35823;&#65281;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25351;&#20196;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation in Continuous Environments (VLN-CE) &#26159;&#19968;&#39033;&#30452;&#35266;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#39564;&#26234;&#33021;&#20219;&#21153;&#12290;&#20195;&#29702;&#20154;&#34987;&#35201;&#27714;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#20302;&#32423;&#21160;&#20316;&#12289;&#36981;&#24490;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#23548;&#33322;&#21040;&#30446;&#26631;&#30446;&#26631;&#12290;&#25152;&#26377;&#25991;&#29486;&#20013;&#30340; VLN-CE &#26041;&#27861;&#37117;&#20551;&#35774;&#35821;&#35328;&#25351;&#20196;&#26159;&#20934;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20154;&#31867;&#32473;&#20986;&#30340;&#25351;&#20196;&#21487;&#33021;&#30001;&#20110;&#19981;&#20934;&#30830;&#30340;&#35760;&#24518;&#25110;&#28151;&#28102;&#32780;&#21253;&#21547;&#31354;&#38388;&#29615;&#22659;&#25551;&#36848;&#20013;&#30340;&#38169;&#35823;&#12290;&#24403;&#21069; VLN-CE &#22522;&#20934;&#27809;&#26377;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#65292;&#20351;&#24471; VLN-CE &#20013;&#30340;&#26368;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#26469;&#33258;&#20154;&#31867;&#29992;&#25143;&#30340;&#38169;&#35823;&#25351;&#20196;&#26102;&#21464;&#24471;&#33030;&#24369;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#20837;&#21508;&#31181;&#31867;&#22411;&#25351;&#20196;&#38169;&#35823;&#32771;&#34385;&#28508;&#22312;&#20154;&#31867;&#21407;&#22240;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#20026;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040; noticeable...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10700v1 Announce Type: cross  Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.06832</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06832
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#36827;&#23637;&#20984;&#26174;&#20986;&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65288;MMKG&#65289;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#26694;&#26550;&#23545;&#20110;&#22312;&#35268;&#27169;&#19978;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#25972;&#21512;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#20943;&#36731;&#30693;&#35782;&#35823;&#35299;&#21644;&#22810;&#27169;&#24577;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#23884;&#20837;MMKG&#20013;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MKGC&#65289;&#21644;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNAG&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#37197;&#22791;&#20102;&#27169;&#24577;&#32423;&#22122;&#22768;&#25513;&#27169;&#65292;&#20197;&#22312;&#30693;&#35782;&#22270;&#20013;&#40065;&#26834;&#22320;&#38598;&#25104;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;MKGC&#21644;MMEA&#37117;&#24341;&#20837;&#29305;&#23450;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24635;&#20849;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#65288;&#19977;&#20010;&#29992;&#20110;MKGC&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06832v1 Announce Type: cross  Abstract: The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18023</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Mirror Cognitive Language Processing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#35748;&#30693;&#20219;&#21153;&#20013;&#23454;&#29616;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#30001;&#20110;LLMs&#26159;&#20174;&#20154;&#31867;&#35821;&#35328;&#35748;&#30693;&#30340;&#22823;&#37327;&#25991;&#26412;&#20135;&#20986;&#20013;&#35757;&#32451;&#20986;&#26469;&#30340;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;LLMs&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65292;&#25110;LLMs&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#31867;&#20284;&#20110;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;LLMs&#34920;&#24449;&#21644;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#65292;&#20197;&#35780;&#20272;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#37319;&#29992;&#34920;&#24449;&#30456;&#20284;&#24615;&#20998;&#26512;&#65288;RSA&#65289;&#26469;&#34913;&#37327;16&#31181;&#20027;&#27969;LLMs&#19982;&#22823;&#33041;fMRI&#20449;&#21495;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#25506;&#35752;&#20102;&#21508;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#27169;&#22411;&#35268;&#27169;&#12289;&#23545;&#40784;&#35757;&#32451;&#12289;&#25351;&#23548;&#38468;&#21152;&#65289;&#23545;LLM-&#22823;&#33041;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#27491;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18023v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively cor
&lt;/p&gt;</description></item><item><title>SelectIT&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#30340;&#39640;&#25928;&#36873;&#25321;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16705</link><description>&lt;p&gt;
SelectIT: &#36890;&#36807;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#33258;&#25105;&#21453;&#24605;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#25351;&#23548;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16705
&lt;/p&gt;
&lt;p&gt;
SelectIT&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#30340;&#39640;&#25928;&#36873;&#25321;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#65288;IT&#65289;&#23545;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#36866;&#24212;&#20154;&#31867;&#20013;&#24515;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#39640;&#36136;&#37327;&#30340;IT&#25968;&#25454;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24120;&#35265;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#65292;&#36825;&#22686;&#21152;&#20102;&#25104;&#26412;&#24182;&#38480;&#21046;&#20102;&#24191;&#27867;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SelectIT&#65292;&#23427;&#21033;&#29992;LLM&#26412;&#36523;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26356;&#26377;&#25928;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;IT&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IT&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#36873;&#25321;&#24615;&#32650;&#39548;&#65288;Selective Alpaca&#65289;&#65292;&#36890;&#36807;&#23558;SelectIT&#24212;&#29992;&#20110;Alpaca-GPT4&#25968;&#25454;&#38598;&#32780;&#21019;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#32650;&#39548;&#36827;&#34892;IT&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;SelectIT&#30340;&#31283;&#20581;&#24615;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16705v1 Announce Type: new  Abstract: Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also b
&lt;/p&gt;</description></item></channel></rss>