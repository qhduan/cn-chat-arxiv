<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;SR$_{\text{LLM}}$&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#20840;&#38754;&#30340;&#23433;&#20840;&#39118;&#38505;&#20998;&#31867;&#27861;&#21644;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#36890;&#36807;&#25351;&#20196;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26377;&#25928;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01399</link><description>&lt;p&gt;
&#24320;&#21457;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; - &#19968;&#20010;&#20840;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Developing Safe and Responsible Large Language Models -- A Comprehensive Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;SR$_{\text{LLM}}$&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#20840;&#38754;&#30340;&#23433;&#20840;&#39118;&#38505;&#20998;&#31867;&#27861;&#21644;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#36890;&#36807;&#25351;&#20196;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26377;&#25928;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20154;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#24615;&#21644;&#39118;&#38505;&#26085;&#30410;&#20851;&#27880;&#65292;&#21457;&#23637;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SR$_{\text{LLM}}$&#65289;&#65292;&#36825;&#20010;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLM&#26469;&#22686;&#24378;&#35821;&#35328;&#29983;&#25104;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;LLM&#23433;&#20840;&#39118;&#38505;&#20998;&#31867;&#27861;&#65292;&#24182;&#21033;&#29992;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19982;&#36825;&#31181;&#20998;&#31867;&#27861;&#30456;&#19968;&#33268;&#12290;SR$_{\text{LLM}}$&#26088;&#22312;&#35782;&#21035;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#20869;&#23481;&#24182;&#20135;&#29983;&#33391;&#24615;&#21464;&#21270;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#19981;&#20165;&#26377;&#25928;&#22320;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#32780;&#19988;&#36164;&#28304;&#39640;&#25928;&#19988;&#26131;&#20110;&#35843;&#25972;&#12290;&#22312;&#25105;&#20204;&#23545;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#19987;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#23433;&#20840;&#20869;&#23481;&#29983;&#25104;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#26045;&#23433;&#20840;&#25514;&#26045;&#21518;&#65292;&#20986;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01399v1 Announce Type: new  Abstract: Given the growing concerns around the safety and risks of Large Language Models (LLMs), it is essential to develop methods for mitigating these issues. We introduce Safe and Responsible Large Language Model (SR$_{\text{LLM}}$) , a model designed to enhance the safety of language generation using LLMs. Our approach incorporates a comprehensive LLM safety risk taxonomy and utilizes a dataset annotated by experts that align with this taxonomy. SR$_{\text{LLM}}$ is designed to identify potentially unsafe content and produce benign variations. It employs instruction-based and parameter-efficient fine-tuning methods, making the model not only effective in enhancing safety but also resource-efficient and straightforward to adjust. Through our testing on five benchmark datasets and two proprietary datasets, we observed notable reductions in the generation of unsafe content. Moreover, following the implementation of safety measures, there was a s
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14746</link><description>&lt;p&gt;
&#25193;&#23637;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14746
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#21363;&#22823;&#37096;&#20998;&#21442;&#25968;&#20026;&#38646;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21363;&#37027;&#20123;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#35757;&#32451;&#25439;&#22833;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20272;&#35745;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#20013;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#19978;&#19979;&#30028;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#65306;(1)&#35201;&#22312;&#35757;&#32451;&#35821;&#26009;&#20013;&#34920;&#31034;&#30340;&#25216;&#33021;&#25968;&#37327;&#32763;&#20493;&#65292;&#38656;&#35201;&#23558;&#35821;&#26009;&#35268;&#27169;&#22823;&#32422;&#25193;&#23637;&#19977;&#21040;&#20116;&#20493;&#65292;(2)&#23545;&#20110;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;$N$&#21644;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;$D$&#28385;&#36275;$N \sim D^{0.58}$&#30340;&#20851;&#31995;&#65292;(3)&#22914;&#26524;&#19968;&#20010;LLM&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#65292;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#20986;&#26032;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#20196;&#38142;&#65288;CoI&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#26469;&#22788;&#29702;&#30001;&#22810;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#25351;&#20196;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22810;&#35821;&#35328;&#25688;&#35201;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11532</link><description>&lt;p&gt;
&#25351;&#20196;&#38142;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#20196;&#38142;&#65288;CoI&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#26469;&#22788;&#29702;&#30001;&#22810;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#25351;&#20196;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22810;&#35821;&#35328;&#25688;&#35201;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19968;&#31995;&#21015;&#22823;&#22411;&#21644;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#23545;&#26410;&#26366;&#35265;&#36807;&#30340;&#20219;&#21153;&#20063;&#36866;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25351;&#20196;&#38142;&#65288;CoI&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#20854;&#20013;&#19968;&#20010;&#25351;&#20196;&#30340;&#36755;&#20986;&#25104;&#20026;&#19979;&#19968;&#20010;&#25351;&#20196;&#30340;&#36755;&#20837;&#65292;&#23601;&#20687;&#19968;&#26465;&#38142;&#26465;&#12290;&#19982;&#35299;&#20915;&#21333;&#19968;&#25351;&#20196;&#20219;&#21153;&#30340;&#20256;&#32479;&#20570;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#40723;&#21169;&#27169;&#22411;&#36880;&#27493;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#30452;&#33267;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;CoI&#35843;&#25972;&#65288;&#21363;&#20351;&#29992;CoI&#25351;&#20196;&#36827;&#34892;&#24494;&#35843;&#65289;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#30001;&#22810;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#25351;&#20196;&#33021;&#21147;&#12290;&#32463;CoI&#35843;&#25972;&#30340;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#19978;&#20063;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#35777;&#26126;....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11532v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with a collection of large and diverse instructions has improved the model's generalization to different tasks, even for unseen tasks. However, most existing instruction datasets include only single instructions, and they struggle to follow complex instructions composed of multiple subtasks (Wang et al., 2023a). In this work, we propose a novel concept of compositional instructions called chain-of-instructions (CoI), where the output of one instruction becomes an input for the next like a chain. Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions) improves the model's ability to handle instructions composed of multiple subtasks. CoI-tuned models also outperformed baseline models on multilingual summarization, demonstratin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDGE&#30340;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#24773;&#24863;&#22686;&#24378;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#28041;&#21450;&#22810;&#31181;&#27169;&#24577;&#30340;&#35773;&#21050;&#23545;&#35805;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#35805;&#35821;&#35760;&#21495;&#23545;&#24773;&#24863;&#30340;&#22810;&#26679;&#25928;&#24212;&#12289;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20449;&#21495;&#19982;BART&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#20197;&#21450;&#35805;&#35821;&#12289;&#35805;&#35821;&#24773;&#24863;&#21644;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03658</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#22686;&#24378;&#24773;&#24863;&#30340;&#22522;&#20110;&#22270;&#30340;&#35773;&#21050;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDGE&#30340;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#24773;&#24863;&#22686;&#24378;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#28041;&#21450;&#22810;&#31181;&#27169;&#24577;&#30340;&#35773;&#21050;&#23545;&#35805;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#35805;&#35821;&#35760;&#21495;&#23545;&#24773;&#24863;&#30340;&#22810;&#26679;&#25928;&#24212;&#12289;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20449;&#21495;&#19982;BART&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#20197;&#21450;&#35805;&#35821;&#12289;&#35805;&#35821;&#24773;&#24863;&#21644;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#35773;&#21050;&#35299;&#37322;&#65288;SED&#65289;&#26159;&#19968;&#39033;&#26032;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#28041;&#21450;&#22810;&#31181;&#27169;&#24577;&#65288;&#21363;&#35805;&#35821;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#65289;&#30340;&#35773;&#21050;&#23545;&#35805;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BART&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#35805;&#35821;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20013;&#23384;&#22312;&#30340;&#24773;&#24863;&#65292;&#32780;&#36825;&#20123;&#24773;&#24863;&#26159;&#35773;&#21050;&#35299;&#37322;&#20013;&#30340;&#37325;&#35201;&#32447;&#32034;&#12290;&#20107;&#23454;&#19978;&#65292;&#30001;&#20110;&#20197;&#19979;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#35805;&#35821;&#35760;&#21495;&#23545;&#24773;&#24863;&#30340;&#22810;&#26679;&#25928;&#24212;&#65307;2&#65289;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20449;&#21495;&#19982;BART&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#65307;3&#65289;&#35805;&#35821;&#12289;&#35805;&#35821;&#24773;&#24863;&#21644;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#65292;&#23558;&#24773;&#24863;&#34701;&#20837;&#20197;&#25552;&#21319;SED&#24615;&#33021;&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#22686;&#24378;&#24773;&#24863;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;EDGE&#12290;
&lt;/p&gt;
&lt;p&gt;
Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which aims to generate a natural language explanation for the given sarcastic dialogue that involves multiple modalities (i.e., utterance, video, and audio). Although existing studies have achieved great success based on the generative pretrained language model BART, they overlook exploiting the sentiments residing in the utterance, video and audio, which are vital clues for sarcasm explanation. In fact, it is non-trivial to incorporate sentiments for boosting SED performance, due to three main challenges: 1) diverse effects of utterance tokens on sentiments; 2) gap between video-audio sentiment signals and the embedding space of BART; and 3) various relations among utterances, utterance sentiments, and video-audio sentiments. To tackle these challenges, we propose a novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation framework, named EDGE. In particular, we first propose a lexicon-guided utterance sen
&lt;/p&gt;</description></item><item><title>AutoPlanBench&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36716;&#25442;PDDL&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#26469;&#35828;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.09830</link><description>&lt;p&gt;
AutoPlanBench: &#20174;PDDL&#33258;&#21160;&#29983;&#25104;LLM&#35268;&#21010;&#22120;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09830
&lt;/p&gt;
&lt;p&gt;
AutoPlanBench&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36716;&#25442;PDDL&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#26469;&#35828;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#65288;&#36923;&#36753;-&#27010;&#29575;&#27169;&#22411;&#65289;&#22312;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPlanBench&#65292;&#19968;&#31181;&#23558;PDDL&#20013;&#30340;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#33258;&#21160;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#20182;&#20219;&#21153;&#20173;&#28982;&#36229;&#20986;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are being increasingly used for planning-style tasks, but their capabilities for planning and reasoning are poorly understood. We present AutoPlanBench, a novel method for automatically converting planning benchmarks written in PDDL into textual descriptions and offer a benchmark dataset created with our method. We show that while the best LLM planners do well on some planning tasks, others remain out of reach of current methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05857</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Summarization with Human Edits. (arXiv:2310.05857v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#33539;&#24335;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#36890;&#29992;&#39046;&#22495;&#25277;&#35937;&#21270;&#25688;&#35201;&#29983;&#25104;&#20013;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24182;&#33719;&#24471;&#20102;&#36229;&#36234;&#20256;&#32479;&#20284;&#28982;&#35757;&#32451;&#30340;&#25688;&#35201;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#36739;&#23569;&#25506;&#32034;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#8212;&#8212;&#20154;&#24037;&#32534;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#8212;&#8212;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#65292;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#21516;&#26102;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22522;&#20934;&#25688;&#35201;&#26469;&#27169;&#25311;&#20154;&#24037;&#32534;&#36753;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#21518;&#33719;&#21462;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#65292;&#20197;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#21453;&#39304;&#30340;&#25506;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#25193;&#23637;&#21040;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;SALT&#22312;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data -Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improv
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.08747</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. (arXiv:2308.08747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08747
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#24403;&#27169;&#22411;&#23398;&#20064;&#26032;&#20449;&#24687;&#26102;&#65292;&#23427;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25506;&#31350;LLMs&#22312;&#25345;&#32493;&#24494;&#35843;&#20013;&#26159;&#21542;&#23384;&#22312;CF&#26159;&#24456;&#26377;&#24847;&#20041;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#39046;&#22495;&#30693;&#35782;&#12289;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#30340;&#35282;&#24230;&#23545;LLMs&#30340;&#36951;&#24536;&#29616;&#35937;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;1b&#21040;7b&#30340;&#33539;&#22260;&#20869;&#65292;LLMs&#26222;&#36941;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#19988;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;mT0&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;BLOOMZ&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65288;&#22914;&#24615;&#21035;&#20559;&#35265;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;LLAMA&#30456;&#27604;&#65292;ALPACA&#22312;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information as it learns new information. As large language models (LLMs) have shown excellent performance, it is interesting to uncover whether CF exists in the continual fine-tuning of LLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs' knowledge, from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments demonstrate that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b. Furthermore, as the scale increases, the severity of forgetting also intensifies. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffers less forgetting and maintains more knowledge. We also observe that LLMs can mitigate language bias (e.g. gender bias) during continual fine-tuning. Moreover, we find that ALPACA can maintain more knowledge and capacity compared with LLAMA du
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10236</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#24615;&#33021;&#31361;&#30772;&#20026;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#38169;&#35823;&#29983;&#25104;&#65292;&#22914;&#34394;&#20551;&#39044;&#27979;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#65292;&#20063;&#24341;&#21457;&#20102;&#23545;LLMs&#21487;&#38752;&#24615;&#30340;&#20005;&#37325;&#20851;&#27880;&#65292;&#23588;&#20854;&#22312;&#23545;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#26377;&#25935;&#24863;&#30340;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24050;&#32463;&#26174;&#31034;&#20986;&#20854;&#22312;&#35299;&#37322;&#19968;&#33324;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20851;&#20110;&#23427;&#26159;&#21542;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26377;&#21161;&#20110;&#25506;&#32034;LLMs&#30340;&#33021;&#21147;&#21644;&#25269;&#21046;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#24320;&#23637;&#20102;&#20851;&#20110;LLMs&#39118;&#38505;&#35780;&#20272;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;12&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;4&#20010;LLMs&#22312;4&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#22312;&#25506;&#32034;LLMs&#33021;&#21147;&#21644;&#23545;&#25239;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08941</link><description>&lt;p&gt;
NTK-&#36817;&#20284;MLP&#34701;&#21512;&#29992;&#20110;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#24050;&#25104;&#20026;&#20027;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24494;&#35843;PLM&#21644;&#36827;&#34892;&#25512;&#29702;&#20063;&#26159;&#26114;&#36149;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#19968;&#20123;&#36890;&#29992;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#37327;&#21270;&#21644;&#33976;&#39311;&#65289;&#26469;&#20943;&#23569;PLM&#24494;&#35843;&#30340;&#35745;&#31639;/&#23384;&#20648;&#24320;&#38144;&#65292;&#20294;&#24456;&#23569;&#26377;&#19968;&#27425;&#24615;&#21387;&#32553;&#25216;&#26415;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#27169;&#22359;&#20013;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;NTK&#36817;&#20284;MLP&#34701;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;PLM&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;MLP&#37325;&#26032;&#35270;&#20026;&#19968;&#26463;&#23376;MLP&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#31867;&#20026;&#32473;&#23450;&#25968;&#37327;&#30340;&#36136;&#24515;&#65292;&#28982;&#21518;&#23558;&#20854;&#24674;&#22797;&#20026;&#21387;&#32553;&#30340;MLP&#65292;&#24182;&#24847;&#22806;&#22320;&#26174;&#31034;&#20986;&#23545;&#21407;&#22987;PLM&#30340;NTK&#36827;&#34892;&#33391;&#22909;&#36817;&#20284;&#30340;&#25928;&#26524;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#39564;&#35777;PLM&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
&lt;/p&gt;</description></item></channel></rss>