<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;QualiCLIP&#65292;&#36890;&#36807;&#36136;&#37327;&#24863;&#30693;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#19981;&#38656;&#35201;&#26631;&#35760;MOS&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.11176</link><description>&lt;p&gt;
&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340;&#36136;&#37327;&#24863;&#30693;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Quality-Aware Image-Text Alignment for Real-World Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;QualiCLIP&#65292;&#36890;&#36807;&#36136;&#37327;&#24863;&#30693;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#19981;&#38656;&#35201;&#26631;&#35760;MOS&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65288;NR-IQA&#65289;&#33268;&#21147;&#20110;&#35774;&#35745;&#19968;&#31181;&#22312;&#27809;&#26377;&#39640;&#36136;&#37327;&#21442;&#32771;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#27979;&#37327;&#22270;&#20687;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#31526;&#21512;&#20154;&#31867;&#24863;&#30693;&#65292;&#22823;&#37096;&#20998;&#26368;&#20808;&#36827;&#30340;NR-IQA&#26041;&#27861;&#20013;&#20381;&#36182;&#26631;&#27880;&#30340;&#20027;&#35266;&#35780;&#20998;&#65288;MOS&#65289;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QualiCLIP&#65288;Quality-aware CLIP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#33258;&#30417;&#30563;&#19981;&#38656;&#35201;&#26631;&#35760;MOS&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#31574;&#30053;&#65292;&#20351;&#24471;CLIP&#29983;&#25104;&#30340;&#34920;&#31034;&#19982;&#22270;&#20687;&#22266;&#26377;&#36136;&#37327;&#30456;&#20851;&#12290;&#20174;&#21407;&#22987;&#22270;&#20687;&#24320;&#22987;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#26029;&#22686;&#21152;&#30340;&#24378;&#24230;&#21512;&#25104;&#22320;&#21155;&#21270;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;CLIP&#26681;&#25454;&#20854;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#21453;&#20041;&#25991;&#26412;&#25552;&#31034;&#30340;&#30456;&#20284;&#24615;&#23545;&#36825;&#20123;&#38477;&#35299;&#22270;&#20687;&#36827;&#34892;&#25490;&#21517;&#65292;&#21516;&#26102;&#20445;&#35777;&#19968;&#33268;&#30340;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11176v1 Announce Type: cross  Abstract: No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods to measure image quality in alignment with human perception when a high-quality reference image is unavailable. The reliance on annotated Mean Opinion Scores (MOS) in the majority of state-of-the-art NR-IQA approaches limits their scalability and broader applicability to real-world scenarios. To overcome this limitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based self-supervised opinion-unaware method that does not require labeled MOS. In particular, we introduce a quality-aware image-text alignment strategy to make CLIP generate representations that correlate with the inherent quality of the images. Starting from pristine images, we synthetically degrade them with increasing levels of intensity. Then, we train CLIP to rank these degraded images based on their similarity to quality-related antonym text prompts, while guaranteeing consistent represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#23545;&#35805;&#20195;&#29702;&#22120;ChatGPT&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#24182;&#20511;&#21161;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#35835;&#30340;&#35780;&#35770;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2401.17477</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#24515;&#29702;&#38556;&#30861;&#65306;&#22522;&#20110;ChatGPT&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting mental disorder on social media: a ChatGPT-augmented explainable approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#23545;&#35805;&#20195;&#29702;&#22120;ChatGPT&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#24182;&#20511;&#21161;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#35835;&#30340;&#35780;&#35770;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#34920;&#36798;&#30340;&#25233;&#37057;&#30151;&#29366;&#30340;&#39057;&#29575;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#65292;&#36843;&#20999;&#38656;&#35201;&#20808;&#36827;&#30340;&#26041;&#27861;&#26469;&#21450;&#26102;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;ChatGPT&#31561;&#23545;&#35805;&#20195;&#29702;&#22120;&#26377;&#25928;&#22320;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24212;&#23545;&#21487;&#35299;&#37322;&#24615;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25513;&#30721;&#27880;&#24847;&#21147;&#25552;&#20379;&#20998;&#31867;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#21487;&#35835;&#24615;&#24378;&#30340;&#35780;&#35770;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26377;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#21487;&#35299;&#37322;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#20570;&#20986;&#36129;&#29486;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the digital era, the prevalence of depressive symptoms expressed on social media has raised serious concerns, necessitating advanced methodologies for timely detection. This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT. In our methodology, explanations are achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a novel self-explanatory model, namely BERT-XDD, capable of providing both classification and explanations via masked attention. The interpretability is further enhanced using ChatGPT to transform technical explanations into human-readable commentaries. By introducing an effective and modular approach for interpretable depression detection, our methodology can contribute to the development of socially responsible digital platforms, fostering early intervention and
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10893</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Location Sensitive Embedding for Knowledge Graph Embedding. (arXiv:2401.10893v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23558;&#30693;&#35782;&#22270;&#35889;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#12289;&#20302;&#32500;&#24230;&#30340;&#31354;&#38388;&#65292;&#26377;&#21161;&#20110;&#25512;&#29702;&#21644;&#34917;&#20840;&#20219;&#21153;&#12290;&#35813;&#39046;&#22495;&#20027;&#35201;&#20998;&#20026;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#21644;&#35821;&#20041;&#21305;&#37197;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#22270;&#35889;&#20013;&#30340;&#8220;&#22836;&#23454;&#20307;&#8221;&#21644;&#8220;&#23614;&#23454;&#20307;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#12290;LSE&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#32780;&#19981;&#20165;&#20165;&#26159;&#24179;&#31227;&#12290;LSE&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#32852;&#31995;&#65292;&#37117;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#19968;&#31181;&#26356;&#31616;&#21270;&#30340;&#21464;&#20307;LSEd&#21033;&#29992;&#23545;&#35282;&#30697;&#38453;&#36827;&#34892;&#21464;&#25442;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#33021;&#12290;&#22312;&#23545;&#22235;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#27979;&#35797;&#20013;&#65292;LSEd&#35201;&#20040;&#34920;&#29616;&#26356;&#22909;&#65292;&#35201;&#20040;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating inference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key challenge in translational distance models is their inability to effectively differentiate between 'head' and 'tail' entities in graphs. To address this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using relation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations of LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more streamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four large-scale datasets for link prediction, LSEd either outperforms or is competitive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2306.17184</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#38382;&#39064;&#65311;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Why can neural language models solve next-word prediction? A mathematical perspective. (arXiv:2306.17184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26041;&#38754;&#35777;&#26126;&#20102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#20851;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#25104;&#21151;&#30340;&#20005;&#26684;&#29702;&#35770;&#35299;&#37322;&#23578;&#26410;&#34987;&#25552;&#20986;&#65292;&#22240;&#20026;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#25511;&#21046;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#30340;&#32452;&#21512;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#21487;&#20197;&#29992;&#26469;&#27169;&#25311;&#33521;&#35821;&#21477;&#23376;&#30340;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#19988;&#38169;&#35823;&#29575;&#20026;&#38646;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#31361;&#20986;&#20102;&#23884;&#20837;&#23618;&#21644;&#20840;&#36830;&#25509;&#32452;&#20214;&#22312;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has revolutionized the field of natural language processing, with neural language models proving to be very effective for next-word prediction. However, a rigorous theoretical explanation for their success in the context of formal language theory has not yet been developed, as it is unclear why neural language models can learn the combinatorial rules that govern the next-word prediction task. In this paper, we study a class of formal languages that can be used to model real-world examples of English sentences. We construct neural language models can solve the next-word prediction task in this context with zero error. Our proof highlights the different roles of the embedding layer and the fully connected component within the neural language model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.15932</link><description>&lt;p&gt;
BUCA&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering. (arXiv:2305.15932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#19988;&#22312;&#33539;&#22260;&#19978;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#38480;&#65292;&#26080;&#30417;&#30563;&#30340;&#24120;&#35782;&#25512;&#29702;(UCR)&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;UCR&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#23558;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;(&#20363;&#22914;&#65292;&#30693;&#35782;&#22270;&#35889;)&#65292;&#20294;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#25490;&#21517;&#26469;&#23436;&#25104;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#21512;&#29702;&#21644;&#19981;&#21512;&#29702;&#30340;&#25991;&#26412;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;KG&#30340;&#29616;&#26377;UCR&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#33410;&#30465;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/probe2/BUCA&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry. Our code is available at https://github.com/probe2/BUCA.
&lt;/p&gt;</description></item></channel></rss>