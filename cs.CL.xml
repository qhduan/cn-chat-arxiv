<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04652</link><description>&lt;p&gt;
Yi: &#30001; 01.AI &#25512;&#20986;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Yi: Open Foundation Models by 01.AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04652
&lt;/p&gt;
&lt;p&gt;
Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Yi&#27169;&#22411;&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#20855;&#26377;&#24378;&#22823;&#22810;&#32500;&#33021;&#21147;&#30340;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;6B&#21644;34B&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#23427;&#20204;&#25193;&#23637;&#20026;&#32842;&#22825;&#27169;&#22411;&#12289;200K&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#35832;&#22914;MMLU&#20043;&#31867;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#25105;&#20204;&#24494;&#35843;&#36807;&#30340;&#32842;&#22825;&#27169;&#22411;&#22312;AlpacaEval&#21644;Chatbot Arena&#31561;&#20027;&#35201;&#35780;&#20272;&#24179;&#21488;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#20154;&#31867;&#20559;&#22909;&#29575;&#12290;&#36890;&#36807;&#20381;&#36182;&#20110;&#25105;&#20204;&#30340;&#21487;&#25193;&#23637;&#36229;&#32423;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#21644;&#32463;&#20856;&#30340;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#35748;&#20026;Yi&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#20854;&#25968;&#25454;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#25105;&#20204;&#30340;&#25968;&#25454;&#24037;&#31243;&#24037;&#20316;&#25152;&#24102;&#26469;&#30340;&#12290;&#23545;&#20110;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#32852;&#30340;&#25968;&#25454;&#21435;&#37325;&#21644;&#36136;&#37327;&#36807;&#28388;&#27969;&#27700;&#32447;&#26500;&#24314;&#20102;3100&#20159;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#35821;&#26009;&#24211;&#30340;&#26631;&#35760;&#12290;&#23545;&#20110;&#24494;&#35843;&#65292;&#25105;&#20204;&#23545;&#23567;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04652v1 Announce Type: cross  Abstract: We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20004;&#31181;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#25688;&#35201;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20854;&#20013;&#26080;&#21442;&#32771;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11414</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#27169;&#24577;&#25688;&#35201;&#30340;&#32454;&#31890;&#24230;&#21487;&#35299;&#37322;&#20107;&#23454;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11414
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20004;&#31181;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#25688;&#35201;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20854;&#20013;&#26080;&#21442;&#32771;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25688;&#35201;&#26088;&#22312;&#29983;&#25104;&#22522;&#20110;&#36755;&#20837;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#20107;&#23454;&#24615;&#36755;&#20986;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35780;&#20272;&#22810;&#27169;&#24577;&#25688;&#35201;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26694;&#26550;&#65288;FALLACIOUS&#65289;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21363;&#22522;&#20110;&#21442;&#32771;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#21644;&#26080;&#21442;&#32771;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26080;&#21442;&#32771;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#19981;&#38656;&#35201;&#22522;&#20934;&#30495;&#20540;&#65292;&#22240;&#27492;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#20854;&#20182;&#25351;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#36890;&#36807;GitHub&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11414v1 Announce Type: new  Abstract: Multimodal summarization aims to generate a concise summary based on the input text and image. However, the existing methods potentially suffer from unfactual output. To evaluate the factuality of multimodal summarization models, we propose two fine-grained and explainable evaluation frameworks (FALLACIOUS) for different application scenarios, i.e. reference-based factuality evaluation framework and reference-free factuality evaluation framework. Notably, the reference-free factuality evaluation framework doesn't need ground truth and hence it has a wider application scenario. To evaluate the effectiveness of the proposed frameworks, we compute the correlation between our frameworks and the other metrics. The experimental results show the effectiveness of our proposed method. We will release our code and dataset via github.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#65292;&#21457;&#29616;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;LLMs&#22312;&#39044;&#27979;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32570;&#20047;&#21608;&#26399;&#24615;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#21516;&#26102;&#25351;&#20986;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#21644;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#37322;&#20041;&#26377;&#21161;&#20110;&#25552;&#21319;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10835</link><description>&lt;p&gt;
LLMs&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#29702;&#35299;&#21644;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#65292;&#21457;&#29616;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;LLMs&#22312;&#39044;&#27979;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32570;&#20047;&#21608;&#26399;&#24615;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#21516;&#26102;&#25351;&#20986;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#21644;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#37322;&#20041;&#26377;&#21161;&#20110;&#25552;&#21319;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#24180;&#26469;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#36805;&#36895;&#21457;&#23637;&#12290;&#20316;&#20026;&#19968;&#31181;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26368;&#36817;&#20174;LLMs&#20013;&#33719;&#24471;&#20102;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#65292;LLMs&#30340;&#20559;&#22909;&#23384;&#22312;&#30740;&#31350;&#31354;&#30333;&#12290;&#36890;&#36807;&#23558;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35768;&#22810;&#29305;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#39044;&#27979;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32570;&#20047;&#21608;&#26399;&#24615;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35201;&#27714;LLMs&#21578;&#30693;&#25968;&#25454;&#38598;&#30340;&#21608;&#26399;&#26469;&#35299;&#37322;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#36755;&#20837;&#31574;&#30053;&#65292;&#21457;&#29616;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#21644;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#37322;&#20041;&#31215;&#26497;&#24433;&#21709;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#27934;&#23519;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10835v1 Announce Type: new  Abstract: Large language models (LLMs) have been applied in many fields with rapid development in recent years. As a classic machine learning task, time series forecasting has recently received a boost from LLMs. However, there is a research gap in the LLMs' preferences in this field. In this paper, by comparing LLMs with traditional models, many properties of LLMs in time series prediction are found. For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity. We explain our findings through designing prompts to require LLMs to tell the period of the datasets. In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases positively affects the predictive performance of LLMs for time series. Overall, this study contributes to insight into the advantages and limitations of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2310.09881</link><description>&lt;p&gt;
&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning with Iterative Demonstration Selection. (arXiv:2310.09881v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#35268;&#27169;&#30340;&#36827;&#23637;&#30340;&#25512;&#21160;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;ICL&#30340;&#24615;&#33021;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#31034;&#33539;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31034;&#33539;&#20316;&#20026;&#19978;&#19979;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#21644;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#25991;&#29486;&#24050;&#32463;&#24378;&#35843;&#20102;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#31034;&#33539;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#26368;&#20248;&#31034;&#33539;&#36873;&#25321;&#32500;&#24230;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#20107;&#23454;&#12290;&#20511;&#37492;&#20004;&#20010;&#32500;&#24230;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;(IDS)&#12290;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;(Zero-shot-CoT)&#65292;IDS&#36845;&#20195;&#22320;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;ICL&#30340;&#31034;&#33539;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;IDS&#22312;&#31034;&#33539;&#36873;&#25321;&#20043;&#21069;&#23558;Zero-shot-CoT&#24212;&#29992;&#20110;&#27979;&#35797;&#26679;&#26412;&#12290;&#36755;&#20986;&#30340;&#25512;&#29702;&#36335;&#24452;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Leveraging the merits of both dimensions, we propose Iterative Demonstration Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is 
&lt;/p&gt;</description></item><item><title>CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16671</link><description>&lt;p&gt;
&#25581;&#31192;CLIP&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16671
&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#19968;&#31181;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#20026;&#29616;&#20195;&#35782;&#21035;&#31995;&#32479;&#21644;&#29983;&#25104;&#27169;&#22411;&#27880;&#20837;&#20102;&#27963;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;CLIP&#25104;&#21151;&#30340;&#20027;&#35201;&#22240;&#32032;&#26159;&#20854;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;CLIP&#21482;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#25968;&#25454;&#21644;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#30340;&#38750;&#24120;&#26377;&#38480;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#20854;&#20182;&#30740;&#31350;&#21162;&#21147;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#36807;&#28388;&#26469;&#37325;&#29616;CLIP&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24847;&#22312;&#25581;&#31034;CLIP&#30340;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#65292;&#24182;&#22312;&#20844;&#24320;&#32473;&#31038;&#21306;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20803;&#25968;&#25454;&#25972;&#29702;&#30340;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;MetaCLIP&#65289;&#12290;MetaCLIP&#36890;&#36807;&#23545;&#20803;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24179;&#34913;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#65288;&#20174;CLIP&#30340;&#27010;&#24565;&#20013;&#24471;&#20986;&#65289;&#20013;&#20135;&#29983;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20005;&#26684;&#38548;&#31163;&#20102;&#27169;&#22411;&#21644;&#35757;&#32451;&#35774;&#32622;&#65292;&#20165;&#19987;&#27880;&#20110;&#25968;&#25454;&#12290;MetaCLIP&#24212;&#29992;&#20110;&#21253;&#21547;400M&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#30340;CommonCrawl&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outper
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.08747</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. (arXiv:2308.08747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08747
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#24403;&#27169;&#22411;&#23398;&#20064;&#26032;&#20449;&#24687;&#26102;&#65292;&#23427;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25506;&#31350;LLMs&#22312;&#25345;&#32493;&#24494;&#35843;&#20013;&#26159;&#21542;&#23384;&#22312;CF&#26159;&#24456;&#26377;&#24847;&#20041;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#39046;&#22495;&#30693;&#35782;&#12289;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#30340;&#35282;&#24230;&#23545;LLMs&#30340;&#36951;&#24536;&#29616;&#35937;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;1b&#21040;7b&#30340;&#33539;&#22260;&#20869;&#65292;LLMs&#26222;&#36941;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#19988;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;mT0&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;BLOOMZ&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65288;&#22914;&#24615;&#21035;&#20559;&#35265;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;LLAMA&#30456;&#27604;&#65292;ALPACA&#22312;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information as it learns new information. As large language models (LLMs) have shown excellent performance, it is interesting to uncover whether CF exists in the continual fine-tuning of LLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs' knowledge, from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments demonstrate that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b. Furthermore, as the scale increases, the severity of forgetting also intensifies. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffers less forgetting and maintains more knowledge. We also observe that LLMs can mitigate language bias (e.g. gender bias) during continual fine-tuning. Moreover, we find that ALPACA can maintain more knowledge and capacity compared with LLAMA du
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24615;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36866;&#24403;&#25351;&#23548;&#21518;&#34920;&#29616;&#20248;&#24322;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#27169;&#22411;&#22312;BEIR&#19978;&#30340;&#25928;&#26524;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.09542</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#25490;&#21517;&#33021;&#21147;&#30740;&#31350;&#8212;&#8212;&#20197;ChatGPT&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. (arXiv:2304.09542v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24615;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36866;&#24403;&#25351;&#23548;&#21518;&#34920;&#29616;&#20248;&#24322;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#27169;&#22411;&#22312;BEIR&#19978;&#30340;&#25928;&#26524;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;remarkable&#33021;&#21147;&#65292;&#33021;&#22815;&#23558;&#19968;&#20123;&#38646;&#26679;&#26412;&#35821;&#35328;&#20219;&#21153;&#25512;&#24191;&#33267;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#21644;GPT-4&#31561;&#29983;&#25104;&#24615;LLMs&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#22312;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32463;&#36807;&#36866;&#24403;&#30340;&#25351;&#23548;&#65292;ChatGPT&#21644;GPT-4&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#19978;&#21462;&#24471;&#31454;&#20105;&#20248;&#21183;&#65292;&#29978;&#33267;&#26377;&#26102;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;GPT-4&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#34920;&#29616;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340;monoT5-3B&#65292;BEIR&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#20248;&#20110;monoT5-3B 2.3&#20010;&#28857;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;Mr.TyDi&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#20248;&#20110;monoT5-3B 2.7&#20010;&#28857;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#23567;&#22411;&#19987;&#38376;&#27169;&#22411;&#65288;&#35757;&#32451;&#20110;10K&#20010;ChatGPT&#29983;&#25104;&#30340;&#25968;&#25454;&#65289;&#22312;BEIR&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22312;400K&#20010;MS MARCO&#27880;&#37322;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;monoT5&#12290;&#20195;&#30721;&#21487;&#22312;www.github.com/sunnwe&#19978;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnwe
&lt;/p&gt;</description></item></channel></rss>