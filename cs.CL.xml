<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>DoraemonGPT&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#31526;&#21495;&#35760;&#24518;&#26469;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.08392</link><description>&lt;p&gt;
DoraemonGPT&#65306;&#26397;&#21521;&#29702;&#35299;&#20855;&#26377;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#22330;&#26223;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08392
&lt;/p&gt;
&lt;p&gt;
DoraemonGPT&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#31526;&#21495;&#35760;&#24518;&#26469;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30001;LLM&#39537;&#21160;&#30340;&#35270;&#35273;&#20195;&#29702;&#20027;&#35201;&#38598;&#20013;&#20110;&#35299;&#20915;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29702;&#35299;&#21160;&#24577;&#22330;&#26223;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#36828;&#31163;&#20687;&#24341;&#23548;&#23398;&#29983;&#36827;&#34892;&#23454;&#39564;&#23460;&#23454;&#39564;&#21644;&#35782;&#21035;&#38169;&#35823;&#36825;&#26679;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;&#32771;&#34385;&#21040;&#35270;&#39057;&#27169;&#24577;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#19981;&#26029;&#21464;&#21270;&#24615;&#36136;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DoraemonGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#32508;&#21512;&#27010;&#24565;&#31616;&#27905;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#38382;&#39064;/&#20219;&#21153;&#30340;&#35270;&#39057;&#65292;DoraemonGPT&#39318;&#20808;&#23558;&#36755;&#20837;&#35270;&#39057;&#36716;&#25442;&#20026;&#23384;&#20648;&#19982;&#20219;&#21153;&#30456;&#20851;&#23646;&#24615;&#30340;&#31526;&#21495;&#23384;&#20648;&#22120;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#34920;&#31034;&#20801;&#35768;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23376;&#20219;&#21153;&#24037;&#20855;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#20174;&#32780;&#20135;&#29983;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;&#37492;&#20110;LLM&#22312;&#28041;&#21450;&#19987;&#19994;&#39046;&#22495;&#65288;&#20363;&#22914;&#20998;&#26512;&#23454;&#39564;&#20013;&#28508;&#22312;&#30340;&#31185;&#23398;&#21407;&#29702;&#65289;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08392v2 Announce Type: replace-cross  Abstract: Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorpor
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;</title><link>https://arxiv.org/abs/2303.14537</link><description>&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#65306;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14537
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36749;&#23398;&#25110;PCA&#26469;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#23618;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#28145;&#24230;&#22686;&#24378;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#22914;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#28145;&#24230;&#22686;&#24378;&#33021;&#22815;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30456;&#24212;&#30340;&#30417;&#30563;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#22686;&#24378;&#20943;&#36731;&#20102;&#23618;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#65292;&#21363;"&#23849;&#28291;"&#24418;&#24335;&#30340;&#38382;&#39064;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21046;&#23450;&#20102;&#19968;&#31181;&#36873;&#25321;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#28145;&#24230;&#22686;&#24378;&#23450;&#20301;&#26356;&#28145;&#23618;&#27425;&#30340;&#23618;&#35201;&#20248;&#20110;&#22686;&#24378;&#36755;&#20837;&#25968;&#25454;&#12290; &#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#32593;&#32476;&#21644;&#27169;&#24577;&#26080;&#20851;&#24615;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
&lt;/p&gt;</description></item></channel></rss>