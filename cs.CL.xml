<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.06833</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#23558;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#21527;&#65311;&#25105;&#20204;&#20855;&#20307;&#25351;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35843;&#33410;&#25351;&#20196;&#30340;&#25216;&#26415;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#65292;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25171;&#24320;&#20102;&#26080;&#25968;&#26032;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#20854;&#20182;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24050;&#24314;&#31435;&#20026;&#35268;&#33539;&#30340;&#22522;&#26412;&#23433;&#20840;&#29305;&#24615;&#65292;&#27604;&#22914;&#25351;&#20196;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#23548;&#33268;&#23427;&#20204;&#21457;&#29983;&#25925;&#38556;&#25110;&#26131;&#21463;&#31532;&#19977;&#26041;&#25805;&#25511;&#21644;&#24178;&#25200;&#65288;&#20363;&#22914;&#36890;&#36807;&#38388;&#25509;&#25552;&#31034;/&#21629;&#20196;&#27880;&#20837;&#65289;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#29978;&#33267;&#27809;&#26377;&#30830;&#20999;&#23450;&#20041;&#36825;&#31181;&#20998;&#31163;&#31350;&#31455;&#24847;&#21619;&#30528;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#27979;&#35797;&#20854;&#36829;&#21453;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#20174;&#27169;&#22411;&#30340;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SEP&#65288;&#24212;&#35813;&#25191;&#34892;&#36824;&#26159;&#22788;&#29702;&#65311;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#20801;&#35768;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 Announce Type: cross  Abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04960</link><description>&lt;p&gt;
SecGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SecGPT: An Execution Isolation Architecture for LLM-Based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25193;&#23637;&#20026;&#31995;&#32479;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#24320;&#22987;&#25903;&#25345;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;LLMs&#30340;&#20107;&#23454;&#19978;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#33258;&#21160;&#25191;&#34892;&#33539;&#24335;&#65306;&#21363;&#65292;&#24212;&#29992;&#31243;&#24207;&#21450;&#20854;&#20132;&#20114;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#30340;&#65292;&#25552;&#20379;&#23545;&#29992;&#25143;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#24182;&#34987;&#20801;&#35768;&#33258;&#30001;&#22320;&#30456;&#20114;&#20132;&#20114;&#20197;&#21450;&#19982;&#31995;&#32479;&#20114;&#21160;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#29983;&#24577;&#31995;&#32479;&#31867;&#20284;&#20110;&#26089;&#26399;&#35745;&#31639;&#24179;&#21488;&#30340;&#35774;&#32622;&#65292;&#22312;&#37027;&#37324;&#24212;&#29992;&#31243;&#24207;&#21644;&#31995;&#32479;&#20043;&#38388;&#32570;&#20047;&#36275;&#22815;&#30340;&#38548;&#31163;&#12290;&#30001;&#20110;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#21487;&#33021;&#19981;&#21487;&#20449;&#65292;&#24182;&#19988;&#21463;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#19981;&#31934;&#30830;&#24615;&#21152;&#21095;&#65292;&#24403;&#21069;&#30340;&#35774;&#35745;&#20250;&#20026;&#29992;&#25143;&#24102;&#26469;&#23433;&#20840;&#21644;&#38544;&#31169;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SecGPT&#65292;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#32531;&#35299;&#30001;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#24341;&#36215;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;SecGPT&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#38548;&#31163;&#24212;&#29992;&#31243;&#24207;&#30340;&#25191;&#34892;&#21644;&#26356;&#22810;&#30340;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04960v1 Announce Type: cross  Abstract: Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applications. These LLM apps leverage the de facto natural language-based automated execution paradigm of LLMs: that is, apps and their interactions are defined in natural language, provided access to user data, and allowed to freely interact with each other and the system. These LLM app ecosystems resemble the settings of earlier computing platforms, where there was insufficient isolation between apps and the system. Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users. In this paper, we propose SecGPT, an architecture for LLM-based systems that aims to mitigate the security and privacy issues that arise with the execution of third-party apps. SecGPT's key idea is to isolate the execution of apps and more pre
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04343</link><description>&lt;p&gt;
&#31169;&#26377;&#38646;&#38454;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#26377;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Private Fine-tuning of Large Language Models with Zeroth-order Optimization. (arXiv:2401.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04343
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#23384;&#22312;&#36829;&#21453;&#38544;&#31169;&#30340;&#39118;&#38505;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21046;&#31639;&#27861;&#31283;&#23450;&#24615;&#26469;&#20943;&#36731;&#38544;&#31169;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;DP-SGD&#21487;&#20197;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#20855;&#26377;&#31169;&#26377;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20294;&#20250;&#24102;&#26469;&#24615;&#33021;&#25439;&#22833;&#21644;&#37325;&#22823;&#24037;&#31243;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#30340;&#38646;&#38454;&#31639;&#27861;SPSA&#20013;&#30340;&#26799;&#24230;&#26041;&#21521;&#22987;&#32456;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#20165;&#20381;&#36182;&#20110;&#31169;&#26377;&#25968;&#25454;&#30340;&#20449;&#24687;&#26159;&#27493;&#38271;&#65292;&#21363;&#19968;&#20010;&#26631;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#23545;&#26631;&#37327;&#27493;&#38271;&#36827;&#34892;&#38544;&#31169;&#22788;&#29702;&#65292;&#36825;&#26159;&#23384;&#20648;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#12290;DP-ZO&#21487;&#20197;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#25110;&#39640;&#26031;&#22122;&#22768;&#26469;&#23454;&#29616;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#25552;&#20379;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#24378;&#22823;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#25991;&#29486;&#20013;&#30340;&#26576;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#26159;&#38169;&#35823;&#30340;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01879</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Theoretical guarantees on the best-of-n alignment policy. (arXiv:2401.01879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#25991;&#29486;&#20013;&#30340;&#26576;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#26159;&#38169;&#35823;&#30340;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#26159;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20174;&#19968;&#20010;&#22522;&#26412;&#31574;&#30053;&#20013;&#25277;&#21462;n&#20010;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#22870;&#21169;&#20989;&#25968;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#65292;&#36873;&#25321;&#25490;&#21517;&#26368;&#39640;&#30340;&#26679;&#26412;&#12290;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#22768;&#31216;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#19982;&#22522;&#26412;&#31574;&#30053;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#31561;&#20110;$\log (n) (n-1)/n$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#35770;&#26029;&#30340;&#19981;&#27491;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21482;&#26159;&#23454;&#38469;KL&#25955;&#24230;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35813;&#19978;&#30028;&#30340;&#32039;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#23427;&#33021;&#25552;&#20379;&#19968;&#20010;&#32039;&#33268;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
A simple and effective method for the alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the base policy is equal to $\log (n) (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes. Finally, we propose a new estimator for the KL divergence and empirically show that it provides a tight approximation through a few examples.
&lt;/p&gt;</description></item></channel></rss>